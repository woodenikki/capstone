{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4b089d",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "This project aims to solve the problem of automatically classifying musical chords as either major or minor using audio input. Chord identification is a key task in music analysis, and automating it can save time on transcription and harmonic analysis. By using machine learning and music information retrieval (MIR) techniques, the goal is to create a tool that helps musicians, producers, and educators analyze music in real-time. The project focuses on making chord recognition more accessible and efficient, benefiting both students and professionals in the music industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a7760",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tools/Methodologies\n",
    "\n",
    "To handle the workflow, I'll use several Python libraries:\n",
    "\n",
    "- [librosa](https://librosa.org/doc/latest/index.html) for extracting audio features, [numpy](https://numpy.org/doc/1.24/reference/index.html#reference) and [pandas](https://pandas.pydata.org/docs/reference/index.html#api) for data manipulation, and os and [Kaggle CLI](https://www.kaggle.com/code/donkeys/kaggle-python-api) to download the data directly into the notebook.\n",
    "- [matplotlib](https://matplotlib.org/stable/api/index.html) and [seaborn](https://seaborn.pydata.org/api.html) for exploring and visualizing features like waveforms and spectrograms.\n",
    "- [scikit-learn](https://scikit-learn.org/stable/api/index.html) for baseline models (e.g., logistic regression, SVM), and [tensorflow](https://www.tensorflow.org/api_docs/python/tf/all_symbols) or [keras](https://keras.io/api/) for building CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d74134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Audio feature extraction\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# for Kaggle CLI\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Machine learning models and utilities\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Deep learning for CNNs\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM, Conv2D, MaxPooling2D, Flatten, Dense, Reshape, Dropout, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a95c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"librosa\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"librosa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bc90b",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The dataset used in this project is sourced from the [Musical Instrument Chord Classification (Audio)](https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification) dataset on Kaggle. It contains audio files `.wav` format of chords played on two instruments: guitar and piano. The raw data has been scraped from various sources and is already available for download on Kaggle, eliminating the need for manual data collection. The dataset is well-suited for this project, as it provides a clear distinction between major and minor chords, which is the focus of the classification task.\n",
    "\n",
    "The features for the model will be extracted from the audio files using techniques such as Mel-frequency cepstral coefficients (MFCCs) or spectrograms, which capture important frequency and temporal information from the audio signals. Although other individuals may have used this dataset for similar chord classification tasks, this project will build upon existing work by focusing specifically on distinguishing between major and minor chords, potentially improving upon current models or exploring new machine learning techniques for this type of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a732e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if replicating project\n",
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff522a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification\n",
      "Dataset downloaded and extracted to: C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\dataset\n"
     ]
    }
   ],
   "source": [
    "# Load kaggle.json credentials\n",
    "api_config_path = os.path.join(os.getcwd(), 'kaggle.json')\n",
    "with open(api_config_path, 'r') as f:\n",
    "    kaggle_config = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_config['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_config['key']\n",
    "\n",
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Ensure the 'dataset' folder exists\n",
    "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Use the Kaggle API to download the dataset\n",
    "api.dataset_download_files('deepcontractor/musical-instrument-chord-classification',\n",
    "                           path=dataset_dir, unzip=True)\n",
    "\n",
    "print(\"Dataset downloaded and extracted to:\", dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e0ac4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_0.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_1.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_10.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_100.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_101.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path             id  label\n",
       "0  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...    Major_0.wav  Major\n",
       "1  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...    Major_1.wav  Major\n",
       "2  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...   Major_10.wav  Major\n",
       "3  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...  Major_100.wav  Major\n",
       "4  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...  Major_101.wav  Major"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base directory where the audio files are stored\n",
    "base_dir = os.path.join(os.getcwd(), 'dataset', 'Audio_Files')\n",
    "\n",
    "# Prepare to collect file details\n",
    "file_details = []\n",
    "\n",
    "# Loop through each category directory ('Major' and 'Minor')\n",
    "for category in ['Major', 'Minor']:\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    \n",
    "    for filename in os.listdir(category_dir):\n",
    "        if filename.endswith('.wav'):\n",
    "            # Full path to file\n",
    "            file_path = os.path.join(category_dir, filename)\n",
    "            # Append the file path, filename (used as ID), and label to the list\n",
    "            file_details.append({'path': file_path, 'id': filename, 'label': category})\n",
    "\n",
    "# Save collected file details as a DataFrame\n",
    "file_data = pd.DataFrame(file_details)\n",
    "\n",
    "file_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730fc1e",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Feature Extraction Functions:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab91ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(signal=None, sr=22050, hop_length=512, n_fft=2048):\n",
    "    if signal is None or not isinstance(signal, np.ndarray):\n",
    "        print(\"Warning: No valid audio signal provided.\")\n",
    "        return {\n",
    "            'chroma': np.full(12, np.nan),\n",
    "            'mfcc': np.full(20, np.nan),\n",
    "            'spectral_centroid': np.nan,\n",
    "            'zero_crossing_rate': np.nan\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        chroma = librosa.feature.chroma_stft(y=signal, sr=sr, hop_length=hop_length, n_fft=n_fft).mean(axis=1)\n",
    "        mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=20, hop_length=hop_length, n_fft=n_fft).mean(axis=1)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=signal, sr=sr, hop_length=hop_length).mean()\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(signal, hop_length=hop_length).mean()\n",
    "\n",
    "        return {\n",
    "            'chroma': chroma,\n",
    "            'mfcc': mfccs,\n",
    "            'spectral_centroid': spectral_centroid,\n",
    "            'zero_crossing_rate': zero_crossing_rate\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature extraction: {e}\")\n",
    "        return {\n",
    "            'chroma': np.full(12, np.nan),\n",
    "            'mfcc': np.full(20, np.nan),\n",
    "            'spectral_centroid': np.nan,\n",
    "            'zero_crossing_rate': np.nan\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63aa0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_harmonics(signal=None, sr=22050, n_fft=2048):\n",
    "    try:\n",
    "        if signal is None or len(signal) == 0:\n",
    "            raise ValueError(\"No audio signal provided.\")\n",
    "\n",
    "        S = np.abs(librosa.stft(signal, n_fft=n_fft))\n",
    "        magnitude = np.mean(S, axis=1)\n",
    "        frequency = np.fft.fftfreq(len(magnitude), 1/sr)\n",
    "        positive_freq_idxs = np.where(frequency >= 0)\n",
    "        positive_freqs = frequency[positive_freq_idxs]\n",
    "        positive_magnitude = magnitude[positive_freq_idxs]\n",
    "\n",
    "        peaks, _ = find_peaks(positive_magnitude, height=np.max(positive_magnitude) * 0.1)\n",
    "        harmonic_frequencies = positive_freqs[peaks]\n",
    "        harmonic_intervals = np.diff(harmonic_frequencies) if len(harmonic_frequencies) > 1 else []\n",
    "\n",
    "        return harmonic_frequencies, harmonic_intervals\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing harmonics for augmented signal: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab083e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(signal=None, sr=22050, n_mels=128, hop_length=512, fixed_length=100):\n",
    "    if signal is None or not isinstance(signal, np.ndarray):\n",
    "        print(\"Warning: No valid audio signal provided for mel-spectrogram extraction.\")\n",
    "        return np.full((fixed_length, n_mels), np.nan)\n",
    "\n",
    "    try:\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        log_mel_spectrogram = log_mel_spectrogram.T\n",
    "\n",
    "        if log_mel_spectrogram.shape[0] < fixed_length:\n",
    "            pad_width = fixed_length - log_mel_spectrogram.shape[0]\n",
    "            log_mel_spectrogram = np.pad(log_mel_spectrogram, ((0, pad_width), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            log_mel_spectrogram = log_mel_spectrogram[:fixed_length, :]\n",
    "\n",
    "        return log_mel_spectrogram\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Mel-spectrogram extraction: {e}\")\n",
    "        return np.full((fixed_length, n_mels), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90a8655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate harmonic ratios with feature toggle\n",
    "def calculate_harmonic_ratios(df, harmonic_prefix='harmonic_', toggle=True):\n",
    "    if not toggle:  # If toggle is False, skip harmonic ratio calculation\n",
    "        return df\n",
    "\n",
    "    harmonic_columns = [col for col in df.columns if harmonic_prefix in col]\n",
    "    harmonic_ratios = []\n",
    "\n",
    "    if len(harmonic_columns) > 1:\n",
    "        for i in range(len(harmonic_columns)):\n",
    "            for j in range(i + 1, len(harmonic_columns)):\n",
    "                col_i = harmonic_columns[i]\n",
    "                col_j = harmonic_columns[j]\n",
    "                ratio_col_name = f'hratio_{i+1}_to_{j+1}'\n",
    "\n",
    "                harmonic_ratio = df[col_i] / df[col_j]\n",
    "                harmonic_ratio.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                harmonic_ratios.append(harmonic_ratio.rename(ratio_col_name))\n",
    "\n",
    "        ratio_df = pd.concat([df] + harmonic_ratios, axis=1)\n",
    "        ratio_df.fillna(0, inplace=True)\n",
    "        return ratio_df\n",
    "    else:\n",
    "        print(\"Not enough harmonic columns in data to compute ratios.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad1429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_signal(path, sr=None):\n",
    "    try:\n",
    "        signal, sr = librosa.load(path, sr=sr)\n",
    "        return signal, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio from {path}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10902aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_harmonic_ratios_from_signal(signal, sr=22050, n_fft=2048):\n",
    "    try:\n",
    "        harmonic_frequencies, _ = find_harmonics(signal=signal, sr=sr, n_fft=n_fft)\n",
    "\n",
    "        if harmonic_frequencies is None or len(harmonic_frequencies) < 2:\n",
    "            return None  # Not enough harmonics to calculate ratios\n",
    "\n",
    "        harmonic_ratios = {}\n",
    "        base_freq = harmonic_frequencies[0]  # First harmonic as the base\n",
    "\n",
    "        for i, freq in enumerate(harmonic_frequencies[1:], start=2):\n",
    "            ratio_key = f'ratio_{i}_to_1'\n",
    "            harmonic_ratios[ratio_key] = freq / base_freq\n",
    "\n",
    "        return harmonic_ratios\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting harmonic ratios: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d4022",
   "metadata": {},
   "source": [
    "#### Running Feature Extraction on Origional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ac4643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_signals(data, feature_toggles):\n",
    "    feature_dict_list = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            # Extract audio signal\n",
    "            signal, sr = extract_audio_signal(row['path'])\n",
    "            if signal is None:\n",
    "                continue  # Skip to next if signal is not available\n",
    "\n",
    "            # Initialize the feature dictionary with basic info\n",
    "            feature_dict = {'id': row['id'], 'Label': row['label'], 'audio_signal': signal}\n",
    "\n",
    "            # Extract features based on toggles\n",
    "            if feature_toggles.get('chroma', False):\n",
    "                feature_dict.update({'chroma': extract_audio_features(signal, sr)['chroma']})\n",
    "            if feature_toggles.get('mfcc', False):\n",
    "                feature_dict.update({'mfcc': extract_audio_features(signal, sr)['mfcc']})\n",
    "            if feature_toggles.get('spectral_centroid', False):\n",
    "                feature_dict['spectral_centroid'] = extract_audio_features(signal, sr)['spectral_centroid']\n",
    "            if feature_toggles.get('zero_crossing_rate', False):\n",
    "                feature_dict['zero_crossing_rate'] = extract_audio_features(signal, sr)['zero_crossing_rate']\n",
    "            if feature_toggles.get('harmonics', False):\n",
    "                harmonics, intervals = find_harmonics(signal, sr)\n",
    "                feature_dict['harmonics'] = harmonics\n",
    "                feature_dict['intervals'] = intervals\n",
    "            if feature_toggles.get('mel_spectrogram', False):\n",
    "                mel_spectrogram = extract_mel_spectrogram(signal, sr)\n",
    "                feature_dict['mel_spectrogram'] = mel_spectrogram.flatten()\n",
    "\n",
    "            # Extract harmonic ratios for major/minor classification\n",
    "            if feature_toggles.get('harmonic_ratios', False):\n",
    "                harmonic_ratios = extract_harmonic_ratios_from_signal(signal, sr)\n",
    "                if harmonic_ratios:\n",
    "                    feature_dict.update(harmonic_ratios)\n",
    "\n",
    "            # Append the dictionary to the list\n",
    "            feature_dict_list.append(feature_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {row['path']}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(feature_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a922d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='librosa')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='librosa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f0888fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-eb235a571815>:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  signal, sr = librosa.load(path, sr=sr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading audio from C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\dataset\\Audio_Files\\Major\\Major_285.wav: \n",
      "Extracted Features DataFrame:\n",
      "              id  Label                                       audio_signal  \\\n",
      "0    Major_0.wav  Major  [-0.004333496, -0.0058898926, -0.0048217773, -...   \n",
      "1    Major_1.wav  Major  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2   Major_10.wav  Major  [-0.004333496, -0.0058898926, -0.0048217773, -...   \n",
      "3  Major_100.wav  Major  [-0.0062561035, -0.008087158, -0.007171631, -0...   \n",
      "4  Major_101.wav  Major  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                              chroma  \\\n",
      "0  [0.7968521, 0.41725653, 0.2998105, 0.39145, 0....   \n",
      "1  [0.7232829, 0.45263815, 0.26252756, 0.23647432...   \n",
      "2  [0.37183326, 0.2676605, 0.1256456, 0.14466327,...   \n",
      "3  [0.39077398, 0.9342459, 0.7632381, 0.55300677,...   \n",
      "4  [0.20732851, 0.4035183, 0.40069222, 0.4611866,...   \n",
      "\n",
      "                                                mfcc  spectral_centroid  \\\n",
      "0  [-338.3117, 257.16064, 37.91532, -19.911049, 0...         731.644753   \n",
      "1  [-404.3833, 179.5291, 68.798615, 6.5934095, -3...         497.091980   \n",
      "2  [-355.2513, 251.06386, 58.341755, -14.42813, -...         608.686444   \n",
      "3  [-336.5666, 255.45099, 25.056515, -28.689547, ...         735.732537   \n",
      "4  [-390.57816, 171.94157, 63.36294, 0.86880636, ...         619.950017   \n",
      "\n",
      "   zero_crossing_rate                                          harmonics  \\\n",
      "0            0.021721  [258.1463414634146, 387.2195121951219, 516.292...   \n",
      "1            0.011334  [387.2195121951219, 516.2926829268292, 645.365...   \n",
      "2            0.020248  [258.1463414634146, 387.2195121951219, 516.292...   \n",
      "3            0.024483  [301.1707317073171, 387.2195121951219, 602.341...   \n",
      "4            0.012351  [301.1707317073171, 387.2195121951219, 688.390...   \n",
      "\n",
      "                                           intervals  \\\n",
      "0  [129.0731707317073, 129.0731707317073, 129.073...   \n",
      "1  [129.0731707317073, 129.07317073170736, 129.07...   \n",
      "2  [129.0731707317073, 129.0731707317073, 215.121...   \n",
      "3  [86.04878048780483, 215.12195121951225, 172.09...   \n",
      "4  [86.04878048780483, 301.1707317073171, 258.146...   \n",
      "\n",
      "                                     mel_spectrogram  ...  ratio_12_to_1  \\\n",
      "0  [-22.629173, -18.381105, -14.328545, -12.83876...  ...       9.000000   \n",
      "1  [-42.334816, -45.12371, -50.84518, -55.982666,...  ...            NaN   \n",
      "2  [-23.402384, -19.154316, -15.101755, -13.61197...  ...      10.166667   \n",
      "3  [-26.984245, -23.075817, -19.678623, -15.04869...  ...       7.285714   \n",
      "4  [-38.758972, -41.832405, -47.83084, -51.69927,...  ...            NaN   \n",
      "\n",
      "   ratio_13_to_1  ratio_14_to_1  ratio_15_to_1  ratio_16_to_1  ratio_17_to_1  \\\n",
      "0      10.166667      12.166667      14.166667            NaN            NaN   \n",
      "1            NaN            NaN            NaN            NaN            NaN   \n",
      "2      10.833333            NaN            NaN            NaN            NaN   \n",
      "3       9.285714      11.142857      13.000000            NaN            NaN   \n",
      "4            NaN            NaN            NaN            NaN            NaN   \n",
      "\n",
      "   ratio_18_to_1  ratio_19_to_1  ratio_20_to_1  ratio_21_to_1  \n",
      "0            NaN            NaN            NaN            NaN  \n",
      "1            NaN            NaN            NaN            NaN  \n",
      "2            NaN            NaN            NaN            NaN  \n",
      "3            NaN            NaN            NaN            NaN  \n",
      "4            NaN            NaN            NaN            NaN  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature Toggles\n",
    "feature_toggles = {\n",
    "    'chroma': True,\n",
    "    'mfcc': True,\n",
    "    'spectral_centroid': True,\n",
    "    'zero_crossing_rate': True,\n",
    "    'harmonics': True,\n",
    "    'mel_spectrogram': True,\n",
    "    'harmonic_ratios': True \n",
    "}\n",
    "\n",
    "# Extract Features for All Data\n",
    "raw_features_df = extract_features_from_signals(file_data, feature_toggles)\n",
    "\n",
    "# Display Resulting DataFrame\n",
    "print(\"Extracted Features DataFrame:\")\n",
    "print(raw_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db18a89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation of Feature Counts in raw_features_df:\n",
      "Feature 'audio_signal':\n",
      "  Zero Count: 6395195\n",
      "  Non-Zero Count: 78320725\n",
      "\n",
      "Feature 'mel_spectrogram':\n",
      "  Zero Count: 571\n",
      "  Non-Zero Count: 10981829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation Step: Count Zero and Non-Zero Values for Raw Data\n",
    "print(\"Validation of Feature Counts in raw_features_df:\")\n",
    "\n",
    "for column in raw_features_df.columns:\n",
    "    # For numeric scalar columns\n",
    "    if pd.api.types.is_numeric_dtype(raw_features_df[column]):\n",
    "        zero_count = (raw_features_df[column] == 0).sum()\n",
    "        non_zero_count = (raw_features_df[column] != 0).sum()\n",
    "\n",
    "        # Print only if there are more than one 'zero count'\n",
    "        if zero_count > 1:\n",
    "            print(f\"Feature '{column}':\")\n",
    "            print(f\"  Zero Count: {zero_count}\")\n",
    "            print(f\"  Non-Zero Count: {non_zero_count}\\n\")\n",
    "\n",
    "    # For list/array columns\n",
    "    else:\n",
    "        zero_count, non_zero_count = 0, 0\n",
    "        for value in raw_features_df[column]:\n",
    "            if isinstance(value, (list, np.ndarray)):\n",
    "                value_array = np.array(value)\n",
    "                zero_count += np.sum(value_array == 0)\n",
    "                non_zero_count += np.sum(value_array != 0)\n",
    "\n",
    "        # Print only if there are more than one 'zero count'\n",
    "        if zero_count > 1:\n",
    "            print(f\"Feature '{column}':\")\n",
    "            print(f\"  Zero Count: {zero_count}\")\n",
    "            print(f\"  Non-Zero Count: {non_zero_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843bf519",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "\n",
    "We will augment the audio data using techniques such as time-stretching, pitch-shifting, and adding noise. The augmented data will then have features extracted in the same way as the original data. We will apply these augmentations to our data to create synthetic data - to even the distribution of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f19ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation functions\n",
    "def pitch_shift(signal, sr, n_steps=4):\n",
    "    return librosa.effects.pitch_shift(signal, sr=sr, n_steps=n_steps)\n",
    "\n",
    "def add_noise(signal, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(signal))\n",
    "    return signal + noise_factor * noise\n",
    "\n",
    "def augment_audio(signal, sr):\n",
    "    augmentations = ['time_stretch', 'pitch_shift', 'add_noise']\n",
    "    augmentation = random.choice(augmentations)\n",
    "\n",
    "    if augmentation == 'time_stretch':\n",
    "        return librosa.effects.time_stretch(signal, rate=1.2)\n",
    "    elif augmentation == 'pitch_shift':\n",
    "        return pitch_shift(signal, sr, n_steps=4)\n",
    "    elif augmentation == 'add_noise':\n",
    "        return add_noise(signal)\n",
    "    else:\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33e68335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count existing samples in the original dataset\n",
    "original_counts = file_data['label'].value_counts()\n",
    "target_count = 500\n",
    "\n",
    "# Determine how many samples to augment for each class\n",
    "augmented_counts = {label: target_count - count if count < target_count else 0 for label, count in original_counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "420df0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_signals(data, feature_toggles):\n",
    "    feature_dict_list = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            # Extract audio signal\n",
    "            signal, sr = extract_audio_signal(row['path'])\n",
    "            if signal is None:\n",
    "                continue  # Skip to next if signal is not available\n",
    "\n",
    "            # Initialize the feature dictionary with basic info\n",
    "            feature_dict = {'id': row['id'], 'Label': row['label'], 'audio_signal': signal}\n",
    "\n",
    "            # Extract features based on toggles\n",
    "            if feature_toggles.get('chroma', False):\n",
    "                chroma_features = extract_audio_features(signal, sr).get('chroma')\n",
    "                if chroma_features is not None:\n",
    "                    feature_dict['chroma'] = chroma_features\n",
    "                else:\n",
    "                    print(f\"Warning: Chromatic features could not be extracted for {row['id']}\")\n",
    "            \n",
    "            if feature_toggles.get('mfcc', False):\n",
    "                mfcc_features = extract_audio_features(signal, sr).get('mfcc')\n",
    "                if mfcc_features is not None:\n",
    "                    feature_dict['mfcc'] = mfcc_features\n",
    "                else:\n",
    "                    print(f\"Warning: MFCC features could not be extracted for {row['id']}\")\n",
    "\n",
    "            if feature_toggles.get('spectral_centroid', False):\n",
    "                spectral_centroid = extract_audio_features(signal, sr).get('spectral_centroid')\n",
    "                feature_dict['spectral_centroid'] = spectral_centroid\n",
    "\n",
    "            if feature_toggles.get('zero_crossing_rate', False):\n",
    "                zero_crossing_rate = extract_audio_features(signal, sr).get('zero_crossing_rate')\n",
    "                feature_dict['zero_crossing_rate'] = zero_crossing_rate\n",
    "\n",
    "            if feature_toggles.get('harmonics', False):\n",
    "                harmonics, intervals = find_harmonics(signal, sr)\n",
    "                if harmonics is not None:\n",
    "                    feature_dict['harmonics'] = harmonics\n",
    "                if intervals is not None:\n",
    "                    feature_dict['intervals'] = intervals\n",
    "\n",
    "            if feature_toggles.get('mel_spectrogram', False):\n",
    "                mel_spectrogram = extract_mel_spectrogram(signal, sr)\n",
    "                if mel_spectrogram is not None:\n",
    "                    feature_dict['mel_spectrogram'] = mel_spectrogram.flatten()\n",
    "\n",
    "            # Extract harmonic ratios for major/minor classification\n",
    "            if feature_toggles.get('harmonic_ratios', False):\n",
    "                harmonic_ratios = extract_harmonic_ratios_from_signal(signal, sr)\n",
    "                if harmonic_ratios:\n",
    "                    feature_dict.update(harmonic_ratios)\n",
    "\n",
    "            # Append the dictionary to the list\n",
    "            feature_dict_list.append(feature_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {row['path']}: {e}\")\n",
    "\n",
    "    # Convert the feature list to a DataFrame\n",
    "    features_df = pd.DataFrame(feature_dict_list)\n",
    "\n",
    "    # Check if the DataFrame contains expected columns, especially the missing ones\n",
    "    print(\"\\nColumns present in extracted features DataFrame:\")\n",
    "    print(features_df.columns)\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9e94ab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-eb235a571815>:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  signal, sr = librosa.load(path, sr=sr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading audio from C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\dataset\\Audio_Files\\Major\\Major_285.wav: \n",
      "\n",
      "Columns present in extracted features DataFrame:\n",
      "Index(['id', 'Label', 'audio_signal', 'chroma', 'mfcc', 'spectral_centroid',\n",
      "       'zero_crossing_rate', 'harmonics', 'intervals', 'mel_spectrogram',\n",
      "       'ratio_2_to_1', 'ratio_3_to_1', 'ratio_4_to_1', 'ratio_5_to_1',\n",
      "       'ratio_6_to_1', 'ratio_7_to_1', 'ratio_8_to_1', 'ratio_9_to_1',\n",
      "       'ratio_10_to_1', 'ratio_11_to_1', 'ratio_12_to_1', 'ratio_13_to_1',\n",
      "       'ratio_14_to_1', 'ratio_15_to_1', 'ratio_16_to_1', 'ratio_17_to_1',\n",
      "       'ratio_18_to_1', 'ratio_19_to_1', 'ratio_20_to_1', 'ratio_21_to_1'],\n",
      "      dtype='object')\n",
      "Extracted Features DataFrame (Augmented Data):\n",
      "              id  Label                                       audio_signal  \\\n",
      "0    Major_0.wav  Major  [-0.004333496, -0.0058898926, -0.0048217773, -...   \n",
      "1    Major_1.wav  Major  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2   Major_10.wav  Major  [-0.004333496, -0.0058898926, -0.0048217773, -...   \n",
      "3  Major_100.wav  Major  [-0.0062561035, -0.008087158, -0.007171631, -0...   \n",
      "4  Major_101.wav  Major  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                              chroma  \\\n",
      "0  [0.7968521, 0.41725653, 0.2998105, 0.39145, 0....   \n",
      "1  [0.7232829, 0.45263815, 0.26252756, 0.23647432...   \n",
      "2  [0.37183326, 0.2676605, 0.1256456, 0.14466327,...   \n",
      "3  [0.39077398, 0.9342459, 0.7632381, 0.55300677,...   \n",
      "4  [0.20732851, 0.4035183, 0.40069222, 0.4611866,...   \n",
      "\n",
      "                                                mfcc  spectral_centroid  \\\n",
      "0  [-338.3117, 257.16064, 37.91532, -19.911049, 0...         731.644753   \n",
      "1  [-404.3833, 179.5291, 68.798615, 6.5934095, -3...         497.091980   \n",
      "2  [-355.2513, 251.06386, 58.341755, -14.42813, -...         608.686444   \n",
      "3  [-336.5666, 255.45099, 25.056515, -28.689547, ...         735.732537   \n",
      "4  [-390.57816, 171.94157, 63.36294, 0.86880636, ...         619.950017   \n",
      "\n",
      "   zero_crossing_rate                                          harmonics  \\\n",
      "0            0.021721  [258.1463414634146, 387.2195121951219, 516.292...   \n",
      "1            0.011334  [387.2195121951219, 516.2926829268292, 645.365...   \n",
      "2            0.020248  [258.1463414634146, 387.2195121951219, 516.292...   \n",
      "3            0.024483  [301.1707317073171, 387.2195121951219, 602.341...   \n",
      "4            0.012351  [301.1707317073171, 387.2195121951219, 688.390...   \n",
      "\n",
      "                                           intervals  \\\n",
      "0  [129.0731707317073, 129.0731707317073, 129.073...   \n",
      "1  [129.0731707317073, 129.07317073170736, 129.07...   \n",
      "2  [129.0731707317073, 129.0731707317073, 215.121...   \n",
      "3  [86.04878048780483, 215.12195121951225, 172.09...   \n",
      "4  [86.04878048780483, 301.1707317073171, 258.146...   \n",
      "\n",
      "                                     mel_spectrogram  ...  ratio_12_to_1  \\\n",
      "0  [-22.629173, -18.381105, -14.328545, -12.83876...  ...       9.000000   \n",
      "1  [-42.334816, -45.12371, -50.84518, -55.982666,...  ...       0.000000   \n",
      "2  [-23.402384, -19.154316, -15.101755, -13.61197...  ...      10.166667   \n",
      "3  [-26.984245, -23.075817, -19.678623, -15.04869...  ...       7.285714   \n",
      "4  [-38.758972, -41.832405, -47.83084, -51.69927,...  ...       0.000000   \n",
      "\n",
      "   ratio_13_to_1  ratio_14_to_1  ratio_15_to_1  ratio_16_to_1  ratio_17_to_1  \\\n",
      "0      10.166667      12.166667      14.166667            0.0            0.0   \n",
      "1       0.000000       0.000000       0.000000            0.0            0.0   \n",
      "2      10.833333       0.000000       0.000000            0.0            0.0   \n",
      "3       9.285714      11.142857      13.000000            0.0            0.0   \n",
      "4       0.000000       0.000000       0.000000            0.0            0.0   \n",
      "\n",
      "   ratio_18_to_1  ratio_19_to_1  ratio_20_to_1  ratio_21_to_1  \n",
      "0            0.0            0.0            0.0            0.0  \n",
      "1            0.0            0.0            0.0            0.0  \n",
      "2            0.0            0.0            0.0            0.0  \n",
      "3            0.0            0.0            0.0            0.0  \n",
      "4            0.0            0.0            0.0            0.0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature Toggles for Augmented Data\n",
    "feature_toggles = {\n",
    "    'chroma': True,\n",
    "    'mfcc': True,\n",
    "    'spectral_centroid': True,\n",
    "    'zero_crossing_rate': True,\n",
    "    'harmonics': True,\n",
    "    'mel_spectrogram': True,\n",
    "    'harmonic_ratios': True\n",
    "}\n",
    "\n",
    "# Extract Features for Augmented Data\n",
    "augmented_features_df = extract_features_from_signals(file_data, feature_toggles)\n",
    "\n",
    "# Replace NaN values with 0 in augmented_features_df\n",
    "augmented_features_df.fillna(0, inplace=True)\n",
    "\n",
    "# Display Resulting DataFrame\n",
    "print(\"Extracted Features DataFrame (Augmented Data):\")\n",
    "print(augmented_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a0328c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All expected columns are present.\n",
      "\n",
      "Checking for missing values in important columns:\n",
      "id: 0 missing values\n",
      "Label: 0 missing values\n",
      "audio_signal: 0 missing values\n",
      "chroma: 0 missing values\n",
      "mfcc: 0 missing values\n",
      "spectral_centroid: 0 missing values\n",
      "zero_crossing_rate: 0 missing values\n",
      "harmonics: 0 missing values\n",
      "intervals: 0 missing values\n",
      "mel_spectrogram: 0 missing values\n",
      "\n",
      "First few rows of the extracted features DataFrame:\n",
      "              id  Label                                       audio_signal  \\\n",
      "0    Major_0.wav  Major  [-0.004333496, -0.0058898926, -0.0048217773, -...   \n",
      "1    Major_1.wav  Major  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2   Major_10.wav  Major  [-0.004333496, -0.0058898926, -0.0048217773, -...   \n",
      "3  Major_100.wav  Major  [-0.0062561035, -0.008087158, -0.007171631, -0...   \n",
      "4  Major_101.wav  Major  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                              chroma  \\\n",
      "0  [0.7968521, 0.41725653, 0.2998105, 0.39145, 0....   \n",
      "1  [0.7232829, 0.45263815, 0.26252756, 0.23647432...   \n",
      "2  [0.37183326, 0.2676605, 0.1256456, 0.14466327,...   \n",
      "3  [0.39077398, 0.9342459, 0.7632381, 0.55300677,...   \n",
      "4  [0.20732851, 0.4035183, 0.40069222, 0.4611866,...   \n",
      "\n",
      "                                                mfcc  spectral_centroid  \\\n",
      "0  [-338.3117, 257.16064, 37.91532, -19.911049, 0...         731.644753   \n",
      "1  [-404.3833, 179.5291, 68.798615, 6.5934095, -3...         497.091980   \n",
      "2  [-355.2513, 251.06386, 58.341755, -14.42813, -...         608.686444   \n",
      "3  [-336.5666, 255.45099, 25.056515, -28.689547, ...         735.732537   \n",
      "4  [-390.57816, 171.94157, 63.36294, 0.86880636, ...         619.950017   \n",
      "\n",
      "   zero_crossing_rate                                          harmonics  \\\n",
      "0            0.021721  [258.1463414634146, 387.2195121951219, 516.292...   \n",
      "1            0.011334  [387.2195121951219, 516.2926829268292, 645.365...   \n",
      "2            0.020248  [258.1463414634146, 387.2195121951219, 516.292...   \n",
      "3            0.024483  [301.1707317073171, 387.2195121951219, 602.341...   \n",
      "4            0.012351  [301.1707317073171, 387.2195121951219, 688.390...   \n",
      "\n",
      "                                           intervals  \\\n",
      "0  [129.0731707317073, 129.0731707317073, 129.073...   \n",
      "1  [129.0731707317073, 129.07317073170736, 129.07...   \n",
      "2  [129.0731707317073, 129.0731707317073, 215.121...   \n",
      "3  [86.04878048780483, 215.12195121951225, 172.09...   \n",
      "4  [86.04878048780483, 301.1707317073171, 258.146...   \n",
      "\n",
      "                                     mel_spectrogram  ...  ratio_12_to_1  \\\n",
      "0  [-22.629173, -18.381105, -14.328545, -12.83876...  ...       9.000000   \n",
      "1  [-42.334816, -45.12371, -50.84518, -55.982666,...  ...       0.000000   \n",
      "2  [-23.402384, -19.154316, -15.101755, -13.61197...  ...      10.166667   \n",
      "3  [-26.984245, -23.075817, -19.678623, -15.04869...  ...       7.285714   \n",
      "4  [-38.758972, -41.832405, -47.83084, -51.69927,...  ...       0.000000   \n",
      "\n",
      "   ratio_13_to_1  ratio_14_to_1  ratio_15_to_1  ratio_16_to_1  ratio_17_to_1  \\\n",
      "0      10.166667      12.166667      14.166667            0.0            0.0   \n",
      "1       0.000000       0.000000       0.000000            0.0            0.0   \n",
      "2      10.833333       0.000000       0.000000            0.0            0.0   \n",
      "3       9.285714      11.142857      13.000000            0.0            0.0   \n",
      "4       0.000000       0.000000       0.000000            0.0            0.0   \n",
      "\n",
      "   ratio_18_to_1  ratio_19_to_1  ratio_20_to_1  ratio_21_to_1  \n",
      "0            0.0            0.0            0.0            0.0  \n",
      "1            0.0            0.0            0.0            0.0  \n",
      "2            0.0            0.0            0.0            0.0  \n",
      "3            0.0            0.0            0.0            0.0  \n",
      "4            0.0            0.0            0.0            0.0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Unique value counts for key features:\n",
      "Label: 2 unique values\n",
      "chroma: 1 unique values\n",
      "mfcc: 1 unique values\n",
      "harmonics: 17 unique values\n",
      "\n",
      "Harmonic ratio columns detected:\n",
      "['ratio_2_to_1', 'ratio_3_to_1', 'ratio_4_to_1', 'ratio_5_to_1', 'ratio_6_to_1', 'ratio_7_to_1', 'ratio_8_to_1', 'ratio_9_to_1', 'ratio_10_to_1', 'ratio_11_to_1', 'ratio_12_to_1', 'ratio_13_to_1', 'ratio_14_to_1', 'ratio_15_to_1', 'ratio_16_to_1', 'ratio_17_to_1', 'ratio_18_to_1', 'ratio_19_to_1', 'ratio_20_to_1', 'ratio_21_to_1']\n"
     ]
    }
   ],
   "source": [
    "# Check if the DataFrame contains expected feature columns\n",
    "expected_columns = [\n",
    "    'id', 'Label', 'audio_signal', 'chroma', 'mfcc', 'spectral_centroid',\n",
    "    'zero_crossing_rate', 'harmonics', 'intervals', 'mel_spectrogram'\n",
    "]\n",
    "missing_columns = [col for col in expected_columns if col not in augmented_features_df.columns]\n",
    "if not missing_columns:\n",
    "    print(\"All expected columns are present.\")\n",
    "else:\n",
    "    print(f\"Missing columns: {missing_columns}\")\n",
    "\n",
    "# Check for missing values in key columns\n",
    "print(\"\\nChecking for missing values in important columns:\")\n",
    "for col in expected_columns:\n",
    "    if col in augmented_features_df.columns:\n",
    "        missing_count = augmented_features_df[col].isna().sum()\n",
    "        print(f\"{col}: {missing_count} missing values\")\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify feature extraction\n",
    "print(\"\\nFirst few rows of the extracted features DataFrame:\")\n",
    "print(augmented_features_df.head())\n",
    "\n",
    "# Check unique counts in key features to verify consistency\n",
    "print(\"\\nUnique value counts for key features:\")\n",
    "for col in ['Label', 'chroma', 'mfcc', 'harmonics']:\n",
    "    if col in augmented_features_df.columns:\n",
    "        # Apply a method to hash the lists/arrays, such as converting them to tuples\n",
    "        unique_count = augmented_features_df[col].apply(lambda x: len(set(tuple(x))) if isinstance(x, (list, np.ndarray)) else x).nunique()\n",
    "        print(f\"{col}: {unique_count} unique values\")\n",
    "\n",
    "# Check if harmonic ratio columns exist, as they were added for classification\n",
    "harmonic_ratio_columns = [col for col in augmented_features_df.columns if 'ratio_' in col]\n",
    "if harmonic_ratio_columns:\n",
    "    print(\"\\nHarmonic ratio columns detected:\")\n",
    "    print(harmonic_ratio_columns)\n",
    "else:\n",
    "    print(\"\\nNo harmonic ratio columns detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "560a10e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation of Feature Counts in augmented_features_df:\n",
      "Feature 'audio_signal':\n",
      "  Zero Count: 6395195\n",
      "  Non-Zero Count: 78320725\n",
      "\n",
      "Feature 'mel_spectrogram':\n",
      "  Zero Count: 571\n",
      "  Non-Zero Count: 10981829\n",
      "\n",
      "Feature 'ratio_5_to_1':\n",
      "  Zero Count: 4\n",
      "  Non-Zero Count: 854\n",
      "\n",
      "Feature 'ratio_6_to_1':\n",
      "  Zero Count: 33\n",
      "  Non-Zero Count: 825\n",
      "\n",
      "Feature 'ratio_7_to_1':\n",
      "  Zero Count: 106\n",
      "  Non-Zero Count: 752\n",
      "\n",
      "Feature 'ratio_8_to_1':\n",
      "  Zero Count: 207\n",
      "  Non-Zero Count: 651\n",
      "\n",
      "Feature 'ratio_9_to_1':\n",
      "  Zero Count: 309\n",
      "  Non-Zero Count: 549\n",
      "\n",
      "Feature 'ratio_10_to_1':\n",
      "  Zero Count: 384\n",
      "  Non-Zero Count: 474\n",
      "\n",
      "Feature 'ratio_11_to_1':\n",
      "  Zero Count: 425\n",
      "  Non-Zero Count: 433\n",
      "\n",
      "Feature 'ratio_12_to_1':\n",
      "  Zero Count: 433\n",
      "  Non-Zero Count: 425\n",
      "\n",
      "Feature 'ratio_13_to_1':\n",
      "  Zero Count: 460\n",
      "  Non-Zero Count: 398\n",
      "\n",
      "Feature 'ratio_14_to_1':\n",
      "  Zero Count: 507\n",
      "  Non-Zero Count: 351\n",
      "\n",
      "Feature 'ratio_15_to_1':\n",
      "  Zero Count: 610\n",
      "  Non-Zero Count: 248\n",
      "\n",
      "Feature 'ratio_16_to_1':\n",
      "  Zero Count: 708\n",
      "  Non-Zero Count: 150\n",
      "\n",
      "Feature 'ratio_17_to_1':\n",
      "  Zero Count: 790\n",
      "  Non-Zero Count: 68\n",
      "\n",
      "Feature 'ratio_18_to_1':\n",
      "  Zero Count: 837\n",
      "  Non-Zero Count: 21\n",
      "\n",
      "Feature 'ratio_19_to_1':\n",
      "  Zero Count: 851\n",
      "  Non-Zero Count: 7\n",
      "\n",
      "Feature 'ratio_20_to_1':\n",
      "  Zero Count: 857\n",
      "  Non-Zero Count: 1\n",
      "\n",
      "Feature 'ratio_21_to_1':\n",
      "  Zero Count: 857\n",
      "  Non-Zero Count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation Step: Count Zero and Non-Zero Values for Augmented Data\n",
    "print(\"Validation of Feature Counts in augmented_features_df:\")\n",
    "\n",
    "for column in augmented_features_df.columns:\n",
    "    # For numeric scalar columns\n",
    "    if pd.api.types.is_numeric_dtype(augmented_features_df[column]):\n",
    "        zero_count = (augmented_features_df[column] == 0).sum()\n",
    "        non_zero_count = (augmented_features_df[column] != 0).sum()\n",
    "\n",
    "        # Print only if there are more than one 'zero count'\n",
    "        if zero_count > 1:\n",
    "            print(f\"Feature '{column}':\")\n",
    "            print(f\"  Zero Count: {zero_count}\")\n",
    "            print(f\"  Non-Zero Count: {non_zero_count}\\n\")\n",
    "\n",
    "    # For list/array columns\n",
    "    else:\n",
    "        zero_count, non_zero_count = 0, 0\n",
    "        for value in augmented_features_df[column]:\n",
    "            if isinstance(value, (list, np.ndarray)):\n",
    "                value_array = np.array(value)\n",
    "                zero_count += np.sum(value_array == 0)\n",
    "                non_zero_count += np.sum(value_array != 0)\n",
    "\n",
    "        # Print only if there are more than one 'zero count'\n",
    "        if zero_count > 1:\n",
    "            print(f\"Feature '{column}':\")\n",
    "            print(f\"  Zero Count: {zero_count}\")\n",
    "            print(f\"  Non-Zero Count: {non_zero_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc736107",
   "metadata": {},
   "source": [
    "## Making things better: Plan to Improve Binary Classification Model for Chord Identification\n",
    "\n",
    "### 1. Data Leakage and Train-Test Split\n",
    "- Ensure all feature extraction, scaling, and augmentations are only applied to training data, not validation/test data.\n",
    "- Make sure train-test split occurs before any transformations or augmentations. Use stratified splitting to maintain balanced classes.\n",
    "\n",
    "### 2. Feature Engineering\n",
    "- Avoid overfitting to specific harmonics or intervals. Select those with general patterns across the dataset.\n",
    "- Focus on musically relevant harmonic ratios (e.g., fundamental vs. third or fifth harmonics).\n",
    "- Use statistical methods (e.g., mutual information, RFE) to select impactful features and reduce noisy/redundant ones.\n",
    "\n",
    "### 3. Augmentation\n",
    "- Use controlled augmentation to avoid introducing too much noise.\n",
    "- Focus on augmentations like time-stretching or adding noise, as pitch-shifting may blur differences between major and minor chords.\n",
    "\n",
    "### 4. Data Balancing\n",
    "- Apply techniques like SMOTE for balancing features or mix augmented and original data proportionally to avoid bias.\n",
    "- Ensure data balance is maintained across train, validation, and test sets.\n",
    "\n",
    "### 5. Mel-Spectrograms and CNN + LSTM Combination\n",
    "- Ensure consistent Mel-spectrogram input sizes by padding or truncating spectrograms.\n",
    "- Experiment with CNN kernel sizes and pooling strategies to improve frequency and temporal feature extraction.\n",
    "\n",
    "### 6. Model Selection and Evaluation\n",
    "- If the model is underperforming for the minor class, use `class_weight` to handle imbalanced classes.\n",
    "- Focus on metrics like F1-score, precision, or recall instead of accuracy to evaluate model performance.\n",
    "- Use Batch Normalization, Dropout, and simpler LSTM models to avoid overfitting before moving to CNN + LSTM.\n",
    "\n",
    "### 7. Alternative Feature Representations\n",
    "- Prioritize features that emphasize differences in harmonic structure, such as the major vs. minor third intervals.\n",
    "- Include delta MFCCs for temporal dynamics.\n",
    "- Consider spectral peaks specifically associated with major or minor chord intervals.\n",
    "\n",
    "### 8. Reducing Complexity and Model Testing\n",
    "- Try simpler models like Random Forests and SVM before CNN + LSTM to see if they can distinguish between chords.\n",
    "- Reduce dimensionality by removing highly correlated features or use PCA for variance retention.\n",
    "\n",
    "### 9. Validation Strategy\n",
    "- Use k-fold cross-validation to assess model generalizability and ensure the model isn't overfitting to specific data splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "751b01a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "Label\n",
      "chroma\n",
      "mfcc\n",
      "spectral_centroid\n",
      "zero_crossing_rate\n",
      "harmonics\n",
      "intervals\n",
      "mel_spectrogram\n"
     ]
    }
   ],
   "source": [
    "for column in augmented_features_df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca575957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
