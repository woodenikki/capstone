{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6c17ba",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "This project aims to solve the problem of automatically classifying musical chords as either major or minor using audio input. Chord identification is a key task in music analysis, and automating it can save time on transcription and harmonic analysis. By using machine learning and music information retrieval (MIR) techniques, the goal is to create a tool that helps musicians, producers, and educators analyze music in real-time. The project focuses on making chord recognition more accessible and efficient, benefiting both students and professionals in the music industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c189c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tools/Methodologies\n",
    "\n",
    "To handle the workflow, I'll use several Python libraries:\n",
    "\n",
    "- [librosa](https://librosa.org/doc/latest/index.html) for extracting audio features, [numpy](https://numpy.org/doc/1.24/reference/index.html#reference) and [pandas](https://pandas.pydata.org/docs/reference/index.html#api) for data manipulation, and os and [Kaggle CLI](https://www.kaggle.com/code/donkeys/kaggle-python-api) to download the data directly into the notebook.\n",
    "- [matplotlib](https://matplotlib.org/stable/api/index.html) and [seaborn](https://seaborn.pydata.org/api.html) for exploring and visualizing features like waveforms and spectrograms.\n",
    "- [scikit-learn](https://scikit-learn.org/stable/api/index.html) for baseline models (e.g., logistic regression, SVM), and [tensorflow](https://www.tensorflow.org/api_docs/python/tf/all_symbols) or [keras](https://keras.io/api/) for building CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f8aac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Audio feature extraction\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# for Kaggle CLI\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Machine learning models and utilities\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Deep learning for CNNs\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM, Conv2D, MaxPooling2D, Flatten, Dense, Reshape, Dropout, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6a81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"librosa\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"librosa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35db81",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The dataset used in this project is sourced from the [Musical Instrument Chord Classification (Audio)](https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification) dataset on Kaggle. It contains audio files `.wav` format of chords played on two instruments: guitar and piano. The raw data has been scraped from various sources and is already available for download on Kaggle, eliminating the need for manual data collection. The dataset is well-suited for this project, as it provides a clear distinction between major and minor chords, which is the focus of the classification task.\n",
    "\n",
    "The features for the model will be extracted from the audio files using techniques such as Mel-frequency cepstral coefficients (MFCCs) or spectrograms, which capture important frequency and temporal information from the audio signals. Although other individuals may have used this dataset for similar chord classification tasks, this project will build upon existing work by focusing specifically on distinguishing between major and minor chords, potentially improving upon current models or exploring new machine learning techniques for this type of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b87a3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if replicating project\n",
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be520ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification\n",
      "Dataset downloaded and extracted to: C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\dataset\n"
     ]
    }
   ],
   "source": [
    "# Load kaggle.json credentials\n",
    "api_config_path = os.path.join(os.getcwd(), 'kaggle.json')\n",
    "with open(api_config_path, 'r') as f:\n",
    "    kaggle_config = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_config['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_config['key']\n",
    "\n",
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Ensure the 'dataset' folder exists\n",
    "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Use the Kaggle API to download the dataset\n",
    "api.dataset_download_files('deepcontractor/musical-instrument-chord-classification',\n",
    "                           path=dataset_dir, unzip=True)\n",
    "\n",
    "print(\"Dataset downloaded and extracted to:\", dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c649a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_0.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_1.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_10.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_100.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_101.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path             id  label\n",
       "0  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...    Major_0.wav  Major\n",
       "1  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...    Major_1.wav  Major\n",
       "2  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...   Major_10.wav  Major\n",
       "3  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...  Major_100.wav  Major\n",
       "4  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...  Major_101.wav  Major"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base directory where the audio files are stored\n",
    "base_dir = os.path.join(os.getcwd(), 'dataset', 'Audio_Files')\n",
    "\n",
    "# Prepare to collect file details\n",
    "file_details = []\n",
    "\n",
    "# Loop through each category directory ('Major' and 'Minor')\n",
    "for category in ['Major', 'Minor']:\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    \n",
    "    for filename in os.listdir(category_dir):\n",
    "        if filename.endswith('.wav'):\n",
    "            # Full path to file\n",
    "            file_path = os.path.join(category_dir, filename)\n",
    "            # Append the file path, filename (used as ID), and label to the list\n",
    "            file_details.append({'path': file_path, 'id': filename, 'label': category})\n",
    "\n",
    "# Save collected file details as a DataFrame\n",
    "file_data = pd.DataFrame(file_details)\n",
    "\n",
    "file_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062b27b",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Feature Extraction Functions:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d71e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(signal=None, sr=22050, hop_length=512, n_fft=2048):\n",
    "    if signal is None or not isinstance(signal, np.ndarray):\n",
    "        print(\"Warning: No valid audio signal provided.\")\n",
    "        return {\n",
    "            'chroma': np.full(12, np.nan),\n",
    "            'mfcc': np.full(20, np.nan),\n",
    "            'spectral_centroid': np.nan,\n",
    "            'zero_crossing_rate': np.nan\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        chroma = librosa.feature.chroma_stft(y=signal, sr=sr, hop_length=hop_length, n_fft=n_fft).mean(axis=1)\n",
    "        mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=20, hop_length=hop_length, n_fft=n_fft).mean(axis=1)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=signal, sr=sr, hop_length=hop_length).mean()\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(signal, hop_length=hop_length).mean()\n",
    "\n",
    "        return {\n",
    "            'chroma': chroma,\n",
    "            'mfcc': mfccs,\n",
    "            'spectral_centroid': spectral_centroid,\n",
    "            'zero_crossing_rate': zero_crossing_rate\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature extraction: {e}\")\n",
    "        return {\n",
    "            'chroma': np.full(12, np.nan),\n",
    "            'mfcc': np.full(20, np.nan),\n",
    "            'spectral_centroid': np.nan,\n",
    "            'zero_crossing_rate': np.nan\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bb9dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_harmonics(signal=None, sr=22050, n_fft=2048):\n",
    "    try:\n",
    "        if signal is None or len(signal) == 0:\n",
    "            raise ValueError(\"No audio signal provided.\")\n",
    "\n",
    "        S = np.abs(librosa.stft(signal, n_fft=n_fft))\n",
    "        magnitude = np.mean(S, axis=1)\n",
    "        frequency = np.fft.fftfreq(len(magnitude), 1/sr)\n",
    "        positive_freq_idxs = np.where(frequency >= 0)\n",
    "        positive_freqs = frequency[positive_freq_idxs]\n",
    "        positive_magnitude = magnitude[positive_freq_idxs]\n",
    "\n",
    "        peaks, _ = find_peaks(positive_magnitude, height=np.max(positive_magnitude) * 0.1)\n",
    "        harmonic_frequencies = positive_freqs[peaks]\n",
    "        harmonic_intervals = np.diff(harmonic_frequencies) if len(harmonic_frequencies) > 1 else []\n",
    "\n",
    "        return harmonic_frequencies, harmonic_intervals\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing harmonics for augmented signal: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74d5243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(signal=None, sr=22050, n_mels=128, hop_length=512, fixed_length=100):\n",
    "    if signal is None or not isinstance(signal, np.ndarray):\n",
    "        print(\"Warning: No valid audio signal provided for mel-spectrogram extraction.\")\n",
    "        return np.full((fixed_length, n_mels), np.nan)\n",
    "\n",
    "    try:\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        log_mel_spectrogram = log_mel_spectrogram.T\n",
    "\n",
    "        if log_mel_spectrogram.shape[0] < fixed_length:\n",
    "            pad_width = fixed_length - log_mel_spectrogram.shape[0]\n",
    "            log_mel_spectrogram = np.pad(log_mel_spectrogram, ((0, pad_width), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            log_mel_spectrogram = log_mel_spectrogram[:fixed_length, :]\n",
    "\n",
    "        return log_mel_spectrogram\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Mel-spectrogram extraction: {e}\")\n",
    "        return np.full((fixed_length, n_mels), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b6d4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate harmonic ratios with feature toggle\n",
    "def calculate_harmonic_ratios(df, harmonic_prefix='harmonic_', toggle=True):\n",
    "    if not toggle:  # If toggle is False, skip harmonic ratio calculation\n",
    "        return df\n",
    "\n",
    "    harmonic_columns = [col for col in df.columns if harmonic_prefix in col]\n",
    "    harmonic_ratios = []\n",
    "\n",
    "    if len(harmonic_columns) > 1:\n",
    "        for i in range(len(harmonic_columns)):\n",
    "            for j in range(i + 1, len(harmonic_columns)):\n",
    "                col_i = harmonic_columns[i]\n",
    "                col_j = harmonic_columns[j]\n",
    "                ratio_col_name = f'harmonic_ratio_{i+1}_to_{j+1}'\n",
    "\n",
    "                harmonic_ratio = df[col_i] / df[col_j]\n",
    "                harmonic_ratio.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                harmonic_ratios.append(harmonic_ratio.rename(ratio_col_name))\n",
    "\n",
    "        ratio_df = pd.concat([df] + harmonic_ratios, axis=1)\n",
    "        ratio_df.fillna(0, inplace=True)\n",
    "        return ratio_df\n",
    "    else:\n",
    "        print(\"Not enough harmonic columns in data to compute ratios.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa143131",
   "metadata": {},
   "source": [
    "#### Running Feature Extraction on Origional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6c58dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_signals(data, feature_toggles):\n",
    "    feature_dict_list = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            signal, sr = librosa.load(row['path'], sr=None)\n",
    "            feature_dict = {'id': row['id'], 'Label': row['label']}\n",
    "\n",
    "            # Extract features based on toggles\n",
    "            if feature_toggles.get('chroma', False):\n",
    "                feature_dict.update({'chroma': extract_audio_features(signal, sr)['chroma']})\n",
    "            if feature_toggles.get('mfcc', False):\n",
    "                feature_dict.update({'mfcc': extract_audio_features(signal, sr)['mfcc']})\n",
    "            if feature_toggles.get('spectral_centroid', False):\n",
    "                feature_dict['spectral_centroid'] = extract_audio_features(signal, sr)['spectral_centroid']\n",
    "            if feature_toggles.get('zero_crossing_rate', False):\n",
    "                feature_dict['zero_crossing_rate'] = extract_audio_features(signal, sr)['zero_crossing_rate']\n",
    "            if feature_toggles.get('harmonics', False):\n",
    "                harmonics, intervals = find_harmonics(signal, sr)\n",
    "                feature_dict['harmonics'] = harmonics\n",
    "                feature_dict['intervals'] = intervals\n",
    "            if feature_toggles.get('mel_spectrogram', False):\n",
    "                mel_spectrogram = extract_mel_spectrogram(signal, sr)\n",
    "                feature_dict['mel_spectrogram'] = mel_spectrogram.flatten()\n",
    "\n",
    "            feature_dict_list.append(feature_dict)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {row['path']}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(feature_dict_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "535d6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='librosa')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='librosa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392c016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Toggles\n",
    "feature_toggles = {\n",
    "    'chroma': True,\n",
    "    'mfcc': True,\n",
    "    'spectral_centroid': True,\n",
    "    'zero_crossing_rate': True,\n",
    "    'harmonics': True,\n",
    "    'mel_spectrogram': True\n",
    "}\n",
    "\n",
    "# Extract Features for All Data\n",
    "raw_features_df = extract_features_from_signals(file_data, feature_toggles)\n",
    "\n",
    "# Display Resulting DataFrame\n",
    "print(\"Extracted Features DataFrame:\")\n",
    "print(raw_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Step: Count Zero and Non-Zero Values for Raw Data\n",
    "print(\"Validation of Feature Counts in raw_features_df:\")\n",
    "\n",
    "for column in raw_features_df.columns:\n",
    "    # For numeric scalar columns\n",
    "    if pd.api.types.is_numeric_dtype(raw_features_df[column]):\n",
    "        zero_count = (raw_features_df[column] == 0).sum()\n",
    "        non_zero_count = (raw_features_df[column] != 0).sum()\n",
    "\n",
    "        # Print only if there are more than one 'zero count'\n",
    "        if zero_count > 1:\n",
    "            print(f\"Feature '{column}':\")\n",
    "            print(f\"  Zero Count: {zero_count}\")\n",
    "            print(f\"  Non-Zero Count: {non_zero_count}\\n\")\n",
    "\n",
    "    # For list/array columns\n",
    "    else:\n",
    "        zero_count, non_zero_count = 0, 0\n",
    "        for value in raw_features_df[column]:\n",
    "            if isinstance(value, (list, np.ndarray)):\n",
    "                value_array = np.array(value)\n",
    "                zero_count += np.sum(value_array == 0)\n",
    "                non_zero_count += np.sum(value_array != 0)\n",
    "\n",
    "        # Print only if there are more than one 'zero count'\n",
    "        if zero_count > 1:\n",
    "            print(f\"Feature '{column}':\")\n",
    "            print(f\"  Zero Count: {zero_count}\")\n",
    "            print(f\"  Non-Zero Count: {non_zero_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9998e",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "\n",
    "We will augment the audio data using techniques such as time-stretching, pitch-shifting, and adding noise. The augmented data will then have features extracted in the same way as the original data. We will apply these augmentations to our data to create synthetic data - to even the distribution of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54195b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation functions\n",
    "def pitch_shift(signal, sr, n_steps=4):\n",
    "    return librosa.effects.pitch_shift(signal, sr=sr, n_steps=n_steps)\n",
    "\n",
    "def add_noise(signal, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(signal))\n",
    "    return signal + noise_factor * noise\n",
    "\n",
    "def augment_audio(signal, sr):\n",
    "    augmentations = ['time_stretch', 'pitch_shift', 'add_noise']\n",
    "    augmentation = random.choice(augmentations)\n",
    "\n",
    "    if augmentation == 'time_stretch':\n",
    "        return librosa.effects.time_stretch(signal, rate=1.2)\n",
    "    elif augmentation == 'pitch_shift':\n",
    "        return pitch_shift(signal, sr, n_steps=4)\n",
    "    elif augmentation == 'add_noise':\n",
    "        return add_noise(signal)\n",
    "    else:\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33a9c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count existing samples in the original dataset\n",
    "original_counts = file_data['label'].value_counts()\n",
    "target_count = 500\n",
    "\n",
    "# Determine how many samples to augment for each class\n",
    "augmented_counts = {label: target_count - count if count < target_count else 0 for label, count in original_counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6846069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation and Feature Extraction Workflow for Augmented Data\n",
    "def augment_and_extract_features(data, target_count, feature_toggles):\n",
    "    augmented_data = []\n",
    "    augmentation_tracker = {label: 0 for label in data['label'].unique()}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        try:\n",
    "            # Skip if augmentation target already met for the class\n",
    "            if augmentation_tracker[row['label']] >= target_count:\n",
    "                continue\n",
    "\n",
    "            # Load the original audio signal\n",
    "            signal, sr = librosa.load(row['path'], sr=None)\n",
    "            if signal is None or len(signal) == 0:\n",
    "                print(f\"Warning: Empty or invalid audio signal for {row['path']}\")\n",
    "                continue\n",
    "\n",
    "            # Apply augmentation\n",
    "            augmented_signal = augment_audio(signal, sr)\n",
    "            if augmented_signal is None or not isinstance(augmented_signal, np.ndarray):\n",
    "                print(f\"Warning: Augmented signal is not valid for {row['path']}\")\n",
    "                continue\n",
    "\n",
    "            # Extract features from the augmented signal\n",
    "            features = extract_audio_features(augmented_signal, sr=sr)\n",
    "            harmonics, intervals = find_harmonics(augmented_signal, sr=sr)\n",
    "            mel_spectrogram = extract_mel_spectrogram(augmented_signal, sr=sr)\n",
    "\n",
    "            # Prepare feature dictionary for augmented data\n",
    "            feature_dict = {\n",
    "                'id': row['id'].replace('.wav', '') + '_aug',\n",
    "                'Label': row['label']\n",
    "            }\n",
    "\n",
    "            # Add features as lists, similar to raw_combined_df\n",
    "            if feature_toggles['chroma']:\n",
    "                feature_dict['chroma'] = features['chroma']\n",
    "\n",
    "            if feature_toggles['mfcc']:\n",
    "                feature_dict['mfcc'] = features['mfcc']\n",
    "\n",
    "            if feature_toggles['spectral_centroid']:\n",
    "                feature_dict['spectral_centroid'] = features['spectral_centroid']\n",
    "\n",
    "            if feature_toggles['zero_crossing_rate']:\n",
    "                feature_dict['zero_crossing_rate'] = features['zero_crossing_rate']\n",
    "\n",
    "            if feature_toggles['harmonics']:\n",
    "                feature_dict['harmonics'] = harmonics\n",
    "                feature_dict['intervals'] = intervals\n",
    "\n",
    "            if feature_toggles['mel_spectrogram']:\n",
    "                feature_dict['mel_spectrogram'] = mel_spectrogram\n",
    "\n",
    "            # Append to augmented data\n",
    "            augmented_data.append(feature_dict)\n",
    "\n",
    "            # Increment the augmentation count for the class\n",
    "            augmentation_tracker[row['label']] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error augmenting and extracting features from {row['path']}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eccabb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-1d57bb54db89>:13: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  signal, sr = librosa.load(row['path'], sr=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error augmenting and extracting features from C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\dataset\\Audio_Files\\Major\\Major_285.wav: \n"
     ]
    }
   ],
   "source": [
    "# Feature Toggles for Augmented Data\n",
    "feature_toggles = {\n",
    "    'chroma': True,\n",
    "    'mfcc': True,\n",
    "    'spectral_centroid': True,\n",
    "    'zero_crossing_rate': True,\n",
    "    'harmonics': True,\n",
    "    'mel_spectrogram': True\n",
    "}\n",
    "\n",
    "# Target Count for Augmentation\n",
    "target_count = 500\n",
    "\n",
    "# Extract Features for Augmented Data\n",
    "augmented_features_df = augment_and_extract_features(file_data, target_count, feature_toggles)\n",
    "\n",
    "# Replace NaN values with 0 in augmented_features_df\n",
    "augmented_features_df.fillna(0, inplace=True)\n",
    "\n",
    "# Display Resulting DataFrame\n",
    "print(\"Extracted Features DataFrame (Augmented Data):\")\n",
    "print(augmented_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Step: Count Zero and Non-Zero Values for Augmented Data\n",
    "print(\"Validation of Feature Counts in augmented_features_df:\")\n",
    "\n",
    "for column in augmented_features_df.columns:\n",
    "    # For numeric scalar columns\n",
    "    if pd.api.types.is_numeric_dtype(augmented_features_df[column]):\n",
    "        zero_count = (augmented_features_df[column] == 0).sum()\n",
    "        non_zero_count = (augmented_features_df[column] != 0).sum()\n",
    "\n",
    "        # Print only if there are more than one 'zero count'\n",
    "        if zero_count > 1:\n",
    "            print(f\"Feature '{column}':\")\n",
    "            print(f\"  Zero Count: {zero_count}\")\n",
    "            print(f\"  Non-Zero Count: {non_zero_count}\\n\")\n",
    "\n",
    "    # For list/array columns\n",
    "    else:\n",
    "        zero_count, non_zero_count = 0, 0\n",
    "        for value in augmented_features_df[column]:\n",
    "            if isinstance(value, (list, np.ndarray)):\n",
    "                value_array = np.array(value)\n",
    "                zero_count += np.sum(value_array == 0)\n",
    "                non_zero_count += np.sum(value_array != 0)\n",
    "\n",
    "        # Print only if there are more than one 'zero count'\n",
    "        if zero_count > 1:\n",
    "            print(f\"Feature '{column}':\")\n",
    "            print(f\"  Zero Count: {zero_count}\")\n",
    "            print(f\"  Non-Zero Count: {non_zero_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef890dc2",
   "metadata": {},
   "source": [
    "## Making things better: Plan to Improve Binary Classification Model for Chord Identification\n",
    "\n",
    "### 1. Data Leakage and Train-Test Split\n",
    "- Ensure all feature extraction, scaling, and augmentations are only applied to training data, not validation/test data.\n",
    "- Make sure train-test split occurs before any transformations or augmentations. Use stratified splitting to maintain balanced classes.\n",
    "\n",
    "### 2. Feature Engineering\n",
    "- Avoid overfitting to specific harmonics or intervals. Select those with general patterns across the dataset.\n",
    "- Focus on musically relevant harmonic ratios (e.g., fundamental vs. third or fifth harmonics).\n",
    "- Use statistical methods (e.g., mutual information, RFE) to select impactful features and reduce noisy/redundant ones.\n",
    "\n",
    "### 3. Augmentation\n",
    "- Use controlled augmentation to avoid introducing too much noise.\n",
    "- Focus on augmentations like time-stretching or adding noise, as pitch-shifting may blur differences between major and minor chords.\n",
    "\n",
    "### 4. Data Balancing\n",
    "- Apply techniques like SMOTE for balancing features or mix augmented and original data proportionally to avoid bias.\n",
    "- Ensure data balance is maintained across train, validation, and test sets.\n",
    "\n",
    "### 5. Mel-Spectrograms and CNN + LSTM Combination\n",
    "- Ensure consistent Mel-spectrogram input sizes by padding or truncating spectrograms.\n",
    "- Experiment with CNN kernel sizes and pooling strategies to improve frequency and temporal feature extraction.\n",
    "\n",
    "### 6. Model Selection and Evaluation\n",
    "- If the model is underperforming for the minor class, use `class_weight` to handle imbalanced classes.\n",
    "- Focus on metrics like F1-score, precision, or recall instead of accuracy to evaluate model performance.\n",
    "- Use Batch Normalization, Dropout, and simpler LSTM models to avoid overfitting before moving to CNN + LSTM.\n",
    "\n",
    "### 7. Alternative Feature Representations\n",
    "- Prioritize features that emphasize differences in harmonic structure, such as the major vs. minor third intervals.\n",
    "- Include delta MFCCs for temporal dynamics.\n",
    "- Consider spectral peaks specifically associated with major or minor chord intervals.\n",
    "\n",
    "### 8. Reducing Complexity and Model Testing\n",
    "- Try simpler models like Random Forests and SVM before CNN + LSTM to see if they can distinguish between chords.\n",
    "- Reduce dimensionality by removing highly correlated features or use PCA for variance retention.\n",
    "\n",
    "### 9. Validation Strategy\n",
    "- Use k-fold cross-validation to assess model generalizability and ensure the model isn't overfitting to specific data splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22e4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
