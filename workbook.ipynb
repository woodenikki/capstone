{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5d4645",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "This project aims to solve the problem of automatically classifying musical chords as either major or minor using audio input. Chord identification is a key task in music analysis, and automating it can save time on transcription and harmonic analysis. By using machine learning and music information retrieval (MIR) techniques, the goal is to create a tool that helps musicians, producers, and educators analyze music in real-time. The project focuses on making chord recognition more accessible and efficient, benefiting both students and professionals in the music industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26229a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tools/Methodologies\n",
    "\n",
    "To handle the workflow, I'll use several Python libraries:\n",
    "\n",
    "- [librosa](https://librosa.org/doc/latest/index.html) for extracting audio features, [numpy](https://numpy.org/doc/1.24/reference/index.html#reference) and [pandas](https://pandas.pydata.org/docs/reference/index.html#api) for data manipulation, and os and [Kaggle CLI](https://www.kaggle.com/code/donkeys/kaggle-python-api) to download the data directly into the notebook.\n",
    "- [matplotlib](https://matplotlib.org/stable/api/index.html) and [seaborn](https://seaborn.pydata.org/api.html) for exploring and visualizing features like waveforms and spectrograms.\n",
    "- [scikit-learn](https://scikit-learn.org/stable/api/index.html) for baseline models (e.g., logistic regression, SVM), and [tensorflow](https://www.tensorflow.org/api_docs/python/tf/all_symbols) or [keras](https://keras.io/api/) for building CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96e50835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Audio feature extraction\n",
    "import librosa\n",
    "\n",
    "# for Kaggle CLI\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning models and utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Deep learning for CNNs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b1a16a",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The dataset used in this project is sourced from the [Musical Instrument Chord Classification (Audio)](https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification) dataset on Kaggle. It contains audio files `.wav` format of chords played on two instruments: guitar and piano. The raw data has been scraped from various sources and is already available for download on Kaggle, eliminating the need for manual data collection. The dataset is well-suited for this project, as it provides a clear distinction between major and minor chords, which is the focus of the classification task.\n",
    "\n",
    "The features for the model will be extracted from the audio files using techniques such as Mel-frequency cepstral coefficients (MFCCs) or spectrograms, which capture important frequency and temporal information from the audio signals. Although other individuals may have used this dataset for similar chord classification tasks, this project will build upon existing work by focusing specifically on distinguishing between major and minor chords, potentially improving upon current models or exploring new machine learning techniques for this type of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfc22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if replicating project\n",
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9007731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification\n",
      "Dataset downloaded and extracted to: C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\dataset\n"
     ]
    }
   ],
   "source": [
    "# Load kaggle.json credentials\n",
    "api_config_path = os.path.join(os.getcwd(), 'kaggle.json')\n",
    "with open(api_config_path, 'r') as f:\n",
    "    kaggle_config = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_config['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_config['key']\n",
    "\n",
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Ensure the 'dataset' folder exists\n",
    "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Use the Kaggle API to download the dataset\n",
    "api.dataset_download_files('deepcontractor/musical-instrument-chord-classification',\n",
    "                           path=dataset_dir, unzip=True)\n",
    "\n",
    "print(\"Dataset downloaded and extracted to:\", dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d32b674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_0.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_1.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_10.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_100.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...</td>\n",
       "      <td>Major_101.wav</td>\n",
       "      <td>Major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path             id  label\n",
       "0  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...    Major_0.wav  Major\n",
       "1  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...    Major_1.wav  Major\n",
       "2  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...   Major_10.wav  Major\n",
       "3  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...  Major_100.wav  Major\n",
       "4  C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\da...  Major_101.wav  Major"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the base directory where the audio files are stored\n",
    "base_dir = os.path.join(os.getcwd(), 'dataset', 'Audio_Files')\n",
    "\n",
    "# Prepare to collect file details\n",
    "file_details = []\n",
    "\n",
    "# Loop through each category directory ('Major' and 'Minor')\n",
    "for category in ['Major', 'Minor']:\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    \n",
    "    for filename in os.listdir(category_dir):\n",
    "        if filename.endswith('.wav'):\n",
    "            # Full path to file\n",
    "            file_path = os.path.join(category_dir, filename)\n",
    "            # Append the file path, filename (used as ID), and label to the list\n",
    "            file_details.append({'path': file_path, 'id': filename, 'label': category})\n",
    "\n",
    "# Save collected file details as a DataFrame\n",
    "data = pd.DataFrame(file_details)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90041cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "The dataset consists of raw .wav files, so no traditional tabular data is available. Preprocessing will involve extracting features like MFCCs, spectrograms, or chroma features to transform the audio into usable data. Challenges include ensuring that these features correctly capture the harmonic information while handling variations in recording quality and instrument type. We estimate several hundred rows of data, each representing an individual chord sample. Visualizations like waveform plots and spectrograms will be used to explore the features that distinguish major from minor chords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8818be3",
   "metadata": {},
   "source": [
    "#### Extract Feature Function\n",
    "- **Audio Loading** with a specified sampling rate (sr)\n",
    "- **Chroma Features**: calculated using the short-time Fourier transform (STFT) of the audio file. Chroma features capture the essence of the pitch content within each octave group - which is vital for chord identification.\n",
    "- **MFCCs**: used to capture the timbral aspects of an audio signal. (This is what makes a song or chord 'feel' a certain way!) The first few coefficients especially provide a good representation of the overall spectral shape which is influenced by the harmonic structure of the chord.\n",
    "- **Feature Averaging**: for both Chroma Features and MFCCs - features are averaged over the time frames to reduce them to a single vector per audio file.\n",
    "- **Error Handling**: for issues with processing the audio file - especially important for our final deployment.\n",
    "\n",
    "#### Encode Labels\n",
    "Our target feature 'label' should be converted into a numerical format.\n",
    "> 0: Major \n",
    "> 1: Minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a3a5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chord_features(file_path, sr=22050, hop_length=512, n_fft=2048):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=sr)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft).mean(axis=1)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20, hop_length=hop_length, n_fft=n_fft).mean(axis=1)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length).mean()\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y, hop_length=hop_length).mean()\n",
    "\n",
    "        # Return a dictionary of features\n",
    "        return {\n",
    "            'chroma': chroma,\n",
    "            'mfcc': mfccs,\n",
    "            'spectral_centroid': spectral_centroid,\n",
    "            'zero_crossing_rate': zero_crossing_rate\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4826b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chroma_0  chroma_1  chroma_2  chroma_3  chroma_4  chroma_5  chroma_6  \\\n",
      "0  0.742913  0.204407  0.173953  0.280547  0.802854  0.172853  0.121578   \n",
      "1  0.729958  0.307365  0.092075  0.152823  0.467690  0.221199  0.145914   \n",
      "2  0.383773  0.197410  0.057362  0.069614  0.227125  0.297883  0.736757   \n",
      "3  0.223433  0.931608  0.578779  0.638870  0.234647  0.322008  0.154993   \n",
      "4  0.137541  0.349761  0.289738  0.445542  0.294066  0.478135  0.327209   \n",
      "\n",
      "   chroma_7  chroma_8  chroma_9  ...    mfcc_14    mfcc_15    mfcc_16  \\\n",
      "0  0.434359  0.100009  0.015909  ...  -9.903286 -18.900269 -12.511242   \n",
      "1  0.392128  0.212576  0.046519  ...  -3.354305  -3.365277  -7.338716   \n",
      "2  0.224346  0.028015  0.237962  ... -21.908617 -10.495011  -6.881425   \n",
      "3  0.370598  0.350831  0.110103  ... -10.768335 -18.064676 -18.102715   \n",
      "4  0.586636  0.273700  0.086248  ...  -6.416312  -7.856865  -4.621236   \n",
      "\n",
      "     mfcc_17    mfcc_18   mfcc_19  spectral_centroid  zero_crossing_rate  \\\n",
      "0 -15.546177 -21.641695 -6.917932         660.890511            0.044776   \n",
      "1 -12.280975 -13.386017 -7.331869         466.196770            0.025587   \n",
      "2 -16.962061 -22.390432 -0.512961         565.448297            0.043005   \n",
      "3  -1.336446   3.123519 -3.986753         694.080903            0.052575   \n",
      "4  -3.370425  -2.344189  1.036036         622.943810            0.029739   \n",
      "\n",
      "          id  Label  \n",
      "0    Major_0  Major  \n",
      "1    Major_1  Major  \n",
      "2   Major_10  Major  \n",
      "3  Major_100  Major  \n",
      "4  Major_101  Major  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "features_data = []\n",
    "\n",
    "# Extracting features for each file and storing them in the list\n",
    "for index, row in data.iterrows():\n",
    "    features = extract_chord_features(row['path'])\n",
    "    if features is not None:\n",
    "        # Flatten the feature dicts for DataFrame construction\n",
    "        feature_dict = {f'chroma_{i}': features['chroma'][i] for i in range(len(features['chroma']))}\n",
    "        feature_dict.update({f'mfcc_{i}': features['mfcc'][i] for i in range(len(features['mfcc']))})\n",
    "        feature_dict['spectral_centroid'] = features['spectral_centroid']\n",
    "        feature_dict['zero_crossing_rate'] = features['zero_crossing_rate']\n",
    "        feature_dict['id'] = row['id'].replace('.wav', '')\n",
    "        feature_dict['Label'] = row['label']\n",
    "        features_data.append(feature_dict)\n",
    "\n",
    "# Creating the DataFrame\n",
    "data_features = pd.DataFrame(features_data)\n",
    "\n",
    "# Display the first few rows of the DataFrame to ensure it is correct\n",
    "print(data_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b94ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "data_features['Label'] = label_encoder.fit_transform(data_features['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab1ad5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "This is a classification problem, with the goal of predicting whether a chord is major or minor. The initial plan is to use a basic model like logistic regression or SVM as a baseline. After extracting features like MFCCs or spectrograms, we will establish the baseline and then experiment with more advanced models like convolutional neural networks (CNNs) to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ce52f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d97c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = data_features.drop(['id', 'Label'], axis=1)\n",
    "y = data_features['Label']\n",
    "\n",
    "# Splitting the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "573e5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "# Building the model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cacd4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN - need 3D arrays (samples, time steps, features)\n",
    "# TODO: should try just extracting these features rather than reshaping 1D to 3D\n",
    "\n",
    "X_train = X_train.values.reshabpe((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.values.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6fc9f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 2s 10ms/step - loss: 5.5842 - accuracy: 0.4934 - val_loss: 0.9891 - val_accuracy: 0.6047\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1.0349 - accuracy: 0.5211 - val_loss: 0.6707 - val_accuracy: 0.6047\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.7053 - accuracy: 0.5459 - val_loss: 0.6691 - val_accuracy: 0.6047\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5735 - val_loss: 0.6734 - val_accuracy: 0.6047\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5866 - val_loss: 0.6766 - val_accuracy: 0.6047\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5837 - val_loss: 0.6755 - val_accuracy: 0.6047\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5881 - val_loss: 0.6762 - val_accuracy: 0.6047\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6848 - accuracy: 0.5881 - val_loss: 0.6767 - val_accuracy: 0.6047\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.5881 - val_loss: 0.6733 - val_accuracy: 0.6047\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6839 - accuracy: 0.5881 - val_loss: 0.6728 - val_accuracy: 0.6047\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6797 - accuracy: 0.5881 - val_loss: 0.6742 - val_accuracy: 0.6047\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.5881 - val_loss: 0.6741 - val_accuracy: 0.6047\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6850 - accuracy: 0.5881 - val_loss: 0.6758 - val_accuracy: 0.6047\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6854 - accuracy: 0.5881 - val_loss: 0.6749 - val_accuracy: 0.6047\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.5881 - val_loss: 0.6723 - val_accuracy: 0.6047\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.5881 - val_loss: 0.6734 - val_accuracy: 0.6047\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6834 - accuracy: 0.5881 - val_loss: 0.6744 - val_accuracy: 0.6047\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6841 - accuracy: 0.5881 - val_loss: 0.6736 - val_accuracy: 0.6047\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6802 - accuracy: 0.5881 - val_loss: 0.6698 - val_accuracy: 0.6047\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5881 - val_loss: 0.6770 - val_accuracy: 0.6047\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999a190",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "To evaluate the model, we will use metrics such as accuracy, precision, recall, and F1-score, focusing on correctly classifying both major and minor chords. The minimum viable product (MVP) will involve building a baseline model (e.g., logistic regression or SVM) to classify the chords with reasonable accuracy. Stretch goals include improving the model using more complex techniques like CNNs or RNNs, and addressing any data imbalances to achieve better generalization and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b6f5ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6956 - accuracy: 0.5349\n",
      "Test Accuracy: 53.49%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792074a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Deployment\n",
    "\n",
    "The results will be delivered through a simple Streamlit web app, where users can upload or record audio files to classify as major or minor chords. The app will provide real-time feedback, displaying the classification result along with confidence scores and visualizations such as waveforms or spectrograms. The app will be hosted on Streamlit Cloud, making it easily accessible and user-friendly for quick chord analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502ae02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
