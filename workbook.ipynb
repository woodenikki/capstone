{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd230940",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "This project aims to solve the problem of automatically classifying musical chords as either major or minor using audio input. Chord identification is a key task in music analysis, and automating it can save time on transcription and harmonic analysis. By using machine learning and music information retrieval (MIR) techniques, the goal is to create a tool that helps musicians, producers, and educators analyze music in real-time. The project focuses on making chord recognition more accessible and efficient, benefiting both students and professionals in the music industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff43366",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tools/Methodologies\n",
    "\n",
    "To handle the workflow, I'll use several Python libraries:\n",
    "\n",
    "- [librosa](https://librosa.org/doc/latest/index.html) for extracting audio features, [numpy](https://numpy.org/doc/1.24/reference/index.html#reference) and [pandas](https://pandas.pydata.org/docs/reference/index.html#api) for data manipulation, and os and [Kaggle CLI](https://www.kaggle.com/code/donkeys/kaggle-python-api) to download the data directly into the notebook.\n",
    "- [matplotlib](https://matplotlib.org/stable/api/index.html) and [seaborn](https://seaborn.pydata.org/api.html) for exploring and visualizing features like waveforms and spectrograms.\n",
    "- [scikit-learn](https://scikit-learn.org/stable/api/index.html) for baseline models (e.g., logistic regression, SVM), and [tensorflow](https://www.tensorflow.org/api_docs/python/tf/all_symbols) or [keras](https://keras.io/api/) for building CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8e857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Audio feature extraction\n",
    "import librosa\n",
    "\n",
    "# for Kaggle CLI\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning models and utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Deep learning for CNNs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb67e7",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "The dataset used in this project is sourced from the [Musical Instrument Chord Classification (Audio)](https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification) dataset on Kaggle. It contains audio files `.wav` format of chords played on two instruments: guitar and piano. The raw data has been scraped from various sources and is already available for download on Kaggle, eliminating the need for manual data collection. The dataset is well-suited for this project, as it provides a clear distinction between major and minor chords, which is the focus of the classification task.\n",
    "\n",
    "The features for the model will be extracted from the audio files using techniques such as Mel-frequency cepstral coefficients (MFCCs) or spectrograms, which capture important frequency and temporal information from the audio signals. Although other individuals may have used this dataset for similar chord classification tasks, this project will build upon existing work by focusing specifically on distinguishing between major and minor chords, potentially improving upon current models or exploring new machine learning techniques for this type of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77ccedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if replicating project\n",
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0db19d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/deepcontractor/musical-instrument-chord-classification\n",
      "Dataset downloaded and extracted to: C:\\Users\\Nik\\Desktop\\code\\Flatiron\\capstone\\dataset\n"
     ]
    }
   ],
   "source": [
    "# Load kaggle.json credentials\n",
    "api_config_path = os.path.join(os.getcwd(), 'kaggle.json')\n",
    "with open(api_config_path, 'r') as f:\n",
    "    kaggle_config = json.load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_config['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_config['key']\n",
    "\n",
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Ensure the 'dataset' folder exists\n",
    "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# Use the Kaggle API to download the dataset\n",
    "api.dataset_download_files('deepcontractor/musical-instrument-chord-classification',\n",
    "                           path=dataset_dir, unzip=True)\n",
    "\n",
    "print(\"Dataset downloaded and extracted to:\", dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6377f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "The dataset consists of raw .wav files, so no traditional tabular data is available. Preprocessing will involve extracting features like MFCCs, spectrograms, or chroma features to transform the audio into usable data. Challenges include ensuring that these features correctly capture the harmonic information while handling variations in recording quality and instrument type. We estimate several hundred rows of data, each representing an individual chord sample. Visualizations like waveform plots and spectrograms will be used to explore the features that distinguish major from minor chords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263bad23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "This is a classification problem, with the goal of predicting whether a chord is major or minor. The initial plan is to use a basic model like logistic regression or SVM as a baseline. After extracting features like MFCCs or spectrograms, we will establish the baseline and then experiment with more advanced models like convolutional neural networks (CNNs) to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dbc76a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "To evaluate the model, we will use metrics such as accuracy, precision, recall, and F1-score, focusing on correctly classifying both major and minor chords. The minimum viable product (MVP) will involve building a baseline model (e.g., logistic regression or SVM) to classify the chords with reasonable accuracy. Stretch goals include improving the model using more complex techniques like CNNs or RNNs, and addressing any data imbalances to achieve better generalization and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5438803",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Deployment\n",
    "\n",
    "The results will be delivered through a simple Streamlit web app, where users can upload or record audio files to classify as major or minor chords. The app will provide real-time feedback, displaying the classification result along with confidence scores and visualizations such as waveforms or spectrograms. The app will be hosted on Streamlit Cloud, making it easily accessible and user-friendly for quick chord analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69314092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
